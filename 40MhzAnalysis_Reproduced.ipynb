{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v1.0 : This notebook is a modified base reproduction an the existing notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 23:36:51.689998: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-28 23:36:51.690190: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-28 23:36:51.692237: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-28 23:36:51.716539: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-28 23:36:52.306830: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import random\n",
    "import sklearn\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pylab\n",
    "from scipy.optimize import curve_fit\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import sklearn.metrics as sk\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please ensure the files are available in the following locations\n",
    "directory_data = \"data\"\n",
    "directory_model = \"models\"\n",
    "h_to_tau_tau_file = \"hToTauTau_13TeV_PU20_filtered.h5\"\n",
    "background_file = \"background_for_training.h5\"\n",
    "A_to_4_l_file = \"Ato4l_lepFilter_13TeV_dataset.h5\"\n",
    "model_name = \"40MHZ_norm_DNN.keras\"\n",
    "hChToTauNu_13TeV_PU20_file = \"hChToTauNu_13TeV_PU20.h5\"\n",
    "leptoquark_LOWMASS_lepFilter_file= \"leptoquark_LOWMASS_lepFilter_13TeV.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_AE(input_dim, h_dim_1, h_dim_2, latent_dim):\n",
    "    # Encoder\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(h_dim_1, activation='relu')(inputs)\n",
    "    x = layers.Dense(h_dim_2, activation='relu')(x)\n",
    "    z = layers.Dense(latent_dim, activation='relu')(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = layers.Dense(h_dim_2, activation='relu')(z)\n",
    "    x = layers.Dense(h_dim_1, activation='relu')(x)\n",
    "    outputs = layers.Dense(input_dim)(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    \"\"\"masked mse\"\"\"\n",
    "    mask = K.cast(K.not_equal(y_true, 0), K.floatx())\n",
    "    squared_difference = K.square(mask * (y_pred - y_true))\n",
    "    return K.mean(squared_difference)\n",
    "\n",
    "def mse_loss(true, prediction):\n",
    "    loss = np.mean(np.square(true - prediction), axis=-1)\n",
    "    return loss\n",
    "\n",
    "def AD_score(y, x):\n",
    "    # masked mse\n",
    "    mask = (y != 0)\n",
    "    _x = x * mask\n",
    "    _y = y * mask\n",
    "    return (mse_loss(_y, _x))\n",
    "\n",
    "def calculate_sensitivity(self,br_rate):\n",
    "    AD=self.AD_cutoff\n",
    "    sensitivity=[]\n",
    "    for i,losses in enumerate(self.signal_loss):\n",
    "        N=len(losses)\n",
    "        n=0\n",
    "        for loss in losses:\n",
    "            if loss>=AD:\n",
    "                n+=1\n",
    "        sen=n/N\n",
    "        sensitivity+=[sen]\n",
    "    self.signal_sensitivity=sensitivity\n",
    "    print(self.signal_sensitivity)\n",
    "\n",
    "\n",
    "def list_datasets(file, path='/'):\n",
    "    print(file[path])\n",
    "    for key in file[path]:\n",
    "        item = file[path + key]\n",
    "        if isinstance(item, h5py.Dataset):\n",
    "            # Print dataset path and shape (which describes its size/dimensions) (Datsets are columns here)\n",
    "            print(f'Dataset: {path + key}, Shape: {item.shape}, Type: {item.dtype}')\n",
    "        elif isinstance(item, h5py.Group):\n",
    "            # It's a group, so recurse into it\n",
    "            list_datasets(file, path + key + '/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Evaluator():\n",
    "  def __init__(self,model_path,backround,signal,title='placeholder',save=False,labels=None):\n",
    "    custom_objects = {'loss_fn': loss_fn}\n",
    "    self.model = load_model(model_path, custom_objects=custom_objects)\n",
    "    self.signal=signal\n",
    "    self.backround=backround\n",
    "    self.br_loss=[]\n",
    "    self.signal_loss=[]\n",
    "    self.backround_outputs=[]\n",
    "    self.signal_outputs=[]\n",
    "    self.title=title\n",
    "    self.saveplots=save\n",
    "    self.labels=labels\n",
    "\n",
    "  def calculate_loss(self,batch_size):\n",
    "    br=self.backround\n",
    "    self.backround_outputs=self.model.predict(br)\n",
    "    self.br_loss=AD_score(self.backround,self.backround_outputs)\n",
    "    for i, batch in enumerate(self.signal):\n",
    "      sr=batch\n",
    "      self.signal_outputs+=[self.model.predict(sr)]\n",
    "      self.signal_loss+=[AD_score(batch,self.signal_outputs[i])]\n",
    "    return [self.br_loss,self.signal_loss]\n",
    "\n",
    "\n",
    "  def histogram(self,bins):\n",
    "    plt.hist(self.br_loss,bins=bins,histtype='step',label='backround num_events:{}'.format(len(self.br_loss)))\n",
    "    for i,batch in enumerate(self.signal_loss):\n",
    "      plt.hist(batch,bins=bins,histtype='step',label=str(self.labels[i+1])+\" num_events:{}\".format(len(batch)))\n",
    "    plt.xlabel('loss')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.yscale('log')\n",
    "    plt.title(\"{}_Hist\".format(self.title))\n",
    "    plt.legend()\n",
    "    if self.saveplots==True:\n",
    "      plt.savefig(\"{}_Hist.png\".format(self.title), format=\"png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "  def ROC(self):\n",
    "    plt.plot(np.linspace(0,1,1000),np.linspace(0,1,1000),'--',label='diagonal')\n",
    "    for j, batch in enumerate(self.signal_loss):\n",
    "      truth=[]\n",
    "      for i in range(len(self.br_loss)):\n",
    "        truth+=[0]\n",
    "      for i in range(len(batch)):\n",
    "        truth+=[1]\n",
    "      ROC_data=np.concatenate((self.br_loss,batch))\n",
    "      fpr,tpr,x=sk.roc_curve(truth,ROC_data)\n",
    "      auc=sk.roc_auc_score(truth,ROC_data)\n",
    "      plt.plot(fpr,tpr,label=self.labels[j+1]+\": \"+str(auc))\n",
    "\n",
    "    plt.xlabel('fpr')\n",
    "    plt.semilogx()\n",
    "    plt.ylabel('trp')\n",
    "    plt.semilogy()\n",
    "    plt.title(\"{}_ROC\".format(self.title))\n",
    "    plt.legend()\n",
    "    if self.saveplots==True:\n",
    "      plt.savefig(\"{}_ROC.png\".format(self.title), format=\"png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "  \n",
    "  def Find_AD_Cutoff(self,br_rate,desired_rate,starting_AD):\n",
    "    N=self.backround.shape[0]\n",
    "    AD_max=starting_AD\n",
    "    AD_List=np.linspace(0,AD_max,num=1000)\n",
    "    best_AD=0\n",
    "    for i,AD in enumerate(np.flip(AD_List)):\n",
    "      n=0\n",
    "      for loss in self.br_loss:\n",
    "        if loss>=AD:\n",
    "          n+=1\n",
    "      sigrate=br_rate*n/N\n",
    "      if sigrate<=desired_rate:\n",
    "        best_AD=AD\n",
    "      if sigrate>desired_rate:\n",
    "        break\n",
    "    self.AD_cutoff=best_AD\n",
    "    return best_AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Training Data Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=h5py.File(os.path.join(directory_data, background_file), 'r')\n",
    "Dataset=np.array(f[\"Particles\"])\n",
    "\n",
    "truthtable=[]\n",
    "\n",
    "threshold=50\n",
    "for i, batch in enumerate(Dataset):\n",
    "  if np.sum(batch[:,0])>=threshold:\n",
    "    truthtable+=[1]\n",
    "  else:\n",
    "    truthtable+=[0]\n",
    "\n",
    "event_pt_br=[]\n",
    "Data_Test_full=Dataset[2000001:3600000,:,:]\n",
    "for j, br_1 in enumerate(Data_Test_full):\n",
    "  event_pt_br+=[np.sum(br_1[:,0])]\n",
    "\n",
    "for i, batch in enumerate(Dataset):\n",
    "  pt_sum=0\n",
    "  for j, particle in enumerate(Dataset[i,:,:]):\n",
    "    if particle[3]!=0:\n",
    "      pt_sum+=particle[0]\n",
    "  for j, particle in enumerate(Dataset[i,:,:]):\n",
    "    particle[0]=particle[0]/pt_sum\n",
    "\n",
    "Data_Train=Dataset[0:2000000,:,0:3]\n",
    "Data_Test=Dataset[2000001:3600000,:,0:3]\n",
    "Test_Truth=truthtable[2000001:3600000]\n",
    "Data_Validate=Dataset[3600001:4000000,:,0:3]\n",
    "\n",
    "Data_Test_Flat=np.reshape(Data_Test,(-1,57))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one set done\n",
      "one set done\n",
      "one set done\n",
      "one set done\n"
     ]
    }
   ],
   "source": [
    "h_to_Tau_Tau=h5py.File(os.path.join(directory_data, h_to_tau_tau_file), 'r')\n",
    "A_to_4_l=h5py.File(os.path.join(directory_data, A_to_4_l_file), 'r')\n",
    "hC_to_Tau_Nu=h5py.File(os.path.join(directory_data, hChToTauNu_13TeV_PU20_file), 'r')\n",
    "lepto=h5py.File(os.path.join(directory_data,leptoquark_LOWMASS_lepFilter_file), 'r')\n",
    "\n",
    "h_tt_set=np.array(h_to_Tau_Tau[\"Particles\"])\n",
    "hC_tn_set=np.array(hC_to_Tau_Nu[\"Particles\"])\n",
    "A_4l_set=np.array(A_to_4_l[\"Particles\"])\n",
    "lepto_set=np.array(lepto[\"Particles\"])\n",
    "sets=[h_tt_set,hC_tn_set, A_4l_set,lepto_set]\n",
    "\n",
    "for k, subset in enumerate(sets):\n",
    "    for i, batch in enumerate(subset):\n",
    "        pt_sum=0\n",
    "    for j, particle in enumerate(subset[i,:,:]):\n",
    "        if particle[3]!=0:\n",
    "            pt_sum+=particle[0]\n",
    "    for j, particle in enumerate(subset[i,:,:]):\n",
    "        particle[0]=particle[0]/pt_sum\n",
    "    print('one set done')\n",
    "\n",
    "normed_signals=[]\n",
    "for j, subset in enumerate(sets):\n",
    "    normed_signals+=[np.reshape(subset[:,:,0:3],(-1,57))]\n",
    "\n",
    "sig_label=['Backround','hC_tn','h_tt','A_4l','leptoquark']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at File Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 group \"/\" (3 members)>\n",
      "Dataset: /Particles, Shape: (55969, 19, 4), Type: float64\n",
      "Dataset: /Particles_Classes, Shape: (4,), Type: |S16\n",
      "Dataset: /Particles_Names, Shape: (4,), Type: |S5\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(os.path.join(directory_data, A_to_4_l_file), 'r') as file:\n",
    "    list_datasets(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21414/50000\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 359us/step"
     ]
    }
   ],
   "source": [
    "evaluation=Model_Evaluator('models/40MHZ_norm_DNN.keras', Data_Test_Flat, normed_signals, title='Normalized 40MHZ DNN AE', save=True, labels=sig_label)\n",
    "evaluation.calculate_loss(1024)\n",
    "evaluation.histogram(bins=100)\n",
    "evaluation.ROC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
